---
title: 'Blog Post number 1'
date: 2012-08-14
permalink: /posts/2012/08/blog-post-1/
tags:
  - cool posts
  - category1
  - category2
---

# Warmup

1.  **Markov Inequality**:
    $$P(X \ge k) \le \frac{EX}{k}, \quad k > 0, X \ge 0$$

2.  **Chebyshev Inequality**:
    $$P(|X-EX| \ge k) \le \frac{\sigma^2}{k^2}$$

# A Straghtforward idea

For any random variable $X \ge 0$ with $EX$, $EX^2$, $\ldots$, $EX^r$
existing for all $k > 0$:
$$P(X \ge k) = P(X^t \ge k^t) \le \min\limits_{t \in [r]} \frac{EX^t}{k^t}$$

**Moment Generating Function (MGF)**: For any random variable $X \ge 0$
with $EX$, $EX^2$, $\ldots$, $EX^r$ existing:
$$Ee^{tX} = 1 + t EX + \frac{t^2}{2}EX^2 + \ldots$$

# Chernoff Inequality

For any random variable $X$ with $Ee^{tX}$ existing for all $k > 0$:
$$P(X \ge k) = P(e^{tX} \ge e^{tk}) \le \inf\limits_{t>0}\{e^{-tk}*Ee^{tX}\}$$

# Simple Concentration Inequality

For random variables $X, X_1, X_2, \ldots, X_n$ being independent and
identically distributed (i.i.d) Bernoulli with $EX = P(X=1) = p$:

$$P\left(\left|\frac{1}{n}\sum_{i=1}^{n}X_i - EX \right| \ge \epsilon \right) \le \frac{Var\left(\frac{1}{n}\sum_{i=1}^{n} X_i\right)}{\epsilon^2} = \frac{p(1-p)}{n\epsilon^2}$$

According to the Central Limit Theorem, the actual convergence rate
should be $e^{-O(n)}$ (Chebyshev's Inequality only uses the second
moment).

# Entropy

**Entropy**: for a probability distribution $(p_1, p_2, \ldots, p_n)$:
$$H(X) := -\sum_{i=1}^{n} p_i \log_2 p_i \text{ (bits)}$$

**Relative Entropy**: for two probability distributions
$P=(p_1,\ldots,p_n)$ and $Q=(q_1,\ldots,q_n)$:
$$D(P||Q) := \sum_i p_i \log_2\frac{p_i}{q_i} \text{ (bits)} \ge 0$$

**Bernoulli Relative Entropy**: for two Bernoulli distributions
$P = (p, 1-p)$ and $Q = (q, 1-q)$:
$$D_B(P||Q) = p\ln \frac{p}{q} + (1-p)\ln\frac{1-p}{1-q}$$

# Chernoff Bound

1.  For i.i.d. Bernoulli random variables $X, X_1, X_2, \ldots, X_n$
    with $EX = P(X=1) = p$:

    $$P\left(\frac{1}{n}\sum_{i=1}^{n}X_i - EX \ge \epsilon \right) = P\left(\sum_{i=1}^{n}X_i \ge n (p + \epsilon) \right) \le \inf_{t>0} E\left(e^{t\sum X_i}\right)e^{-nt(p+\epsilon)}$$

    of which

    $$E\bigl(e^{t\sum X_i}\bigr) = E\bigl[\prod e^{tX_i}\bigr] = \prod Ee^{tX_i} = \bigl(Ee^{tX}\bigr)^n = \bigl[pe^t + (1-p)\bigr]^n$$

    so we have that the right side can be written in:

    $$\inf\limits_{t>0} (pe^t+1-p)^n e^{-nt(p+\epsilon)} = e^{-nD_B(p + \epsilon || p)}$$

2.  For i.i.d. random variables $X, X_1, X_2, \ldots, X_n$ with
    $X \in [0,1],\ EX=p \in [0,1]$:

    $$Ee^{tX} = Ee^{t[1 * X + 0*(1-X)]} \le E[Xe^t + (1-X)] = pe^t +1 - p$$
    the rest proof is similar to that in the first one.

3.  For random variables $X, X_1, X_2, \ldots, X_n$, they are
    independent but not necessarily identically distributed. And we have
    $X_i \in [0,1],\ EX_i = p_i \in [0,1],\ p := \frac{1}{n}\sum\limits_{i}p_i$.
    The only difference with the first one is that:

    $$Ee^{t\sum X_i} = \prod\limits_{i=1}^n Ee^{tX_i} \le \prod(p_ie^t + 1 - p_i) \mathop{\le}\limits^{log + jensen} (pe^t + 1 -p)^n$$

4.  Further simplify the result: **Additive Chernoff Bound**

    $$e^{-nD_B(p + \epsilon || p)} \le e^{-2n\epsilon^2}$$

# Hoeffding Inequality

For independent random variables $X_1, X_2, \ldots, X_N$ where
$X_i \in [a_i, b_i]$ and $\mu := E\left(\frac{1}{n}\sum X_i\right)$:

$$P\left(\frac{1}{n}\sum_{i=1}^n X_i - \mu \ge \epsilon\right) \le \exp\left(-\frac{2n^2\epsilon^2}{\sum_{i}(b_i-a_i)^2}\right)$$

# Draw w/o Replacement

Assume: $a_1, a_2, \cdots, a_N \in \{0, 1\}$. Randomly draw n items from
them.

1.  Draw with replacement (solved)

2.  Draw without replacement (more likely to concentrate)

Let $X_i$ be the r.v. obtained from draw with replacement, $Y_i$ from
draw without replacement. We only need to prove that
$Ee^{t\sum Y_i} \le Ee^{t\sum X_i}$, which means:

$$1 + tE(\sum Y_i) + \frac{t^2}{2}E(\sum\limits_{i,j}Y_iY_j) + \cdots \le 1 + tE(\sum X_i) + \frac{t^2}{2}E(\sum\limits_{i,j}X_iX_j) + \cdots$$

And:

$$E(Y_iY_j) = P(Y_i = 1 \wedge Y_j = 1) \le P(X_i = 1 \wedge X_j = 1) = E(X_iX_j)$$

# McDiarmid's Lemma

Assume $f(x_1, x_2, \ldots, x_n)$ is a stable function, that is, for all
$x_1, \ldots, x_n$ and for all $i$,
$|f(x_1, \ldots, x_i, \ldots, x_n) - f(x_1, \ldots, x_i^{'}, \ldots, x_n)| \le c_i$.
Then, for independent random variables $X_1, \ldots, X_n$:

$$P\left(f(X_1,\ldots,X_n) - Ef(X_1,\ldots,X_n) \ge \epsilon\right) \le \exp\left(-\frac{\epsilon^2}{\sum_{i=1}^n c_i^2}\right)$$

# Recall

A more common way to use the **concentration inequality** is that:

let $\delta=2e^{-2n\epsilon^2}$, as
$P(|\frac{1}{n}\sum_{i=1}^n x_i - p | \geq \epsilon) \leq 2e^{-2n\epsilon^2}$,
we can declare that

$$|\frac{1}{n}\sum_{i=1}^n x_i - p | \leq \sqrt{\frac{\ln \frac{2}{\delta}}{2n}} = O(\sqrt{\ln \frac{1}{\delta}{n}})$$

with probability at least $1-\delta$.

Headings are cool
======

You can have many headings
======

Aren't headings cool?
------